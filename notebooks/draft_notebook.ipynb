{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import pytest\n",
    "\n",
    "from rwd_analytics.cohort import CohortBuilder\n",
    "from rwd_analytics.features_selection import FeaturesSelection, time_at_risk, get_features_scores\n",
    "from rwd_analytics.lookups import Descendants, ConceptInfo, ConceptRelationship, ComorbidConditions, Ingredient\n",
    "from rwd_analytics.treatment_line import last_activity_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "person = pd.DataFrame({\n",
    "    'person_id':[1, 2, 3, 4, 5],\n",
    "    'gender_concept_id':[8532, 8507, 8532, 8507, 8507],\n",
    "    'year_of_birth':[1990, 2000, 2010, 1970, 1960]\n",
    "})\n",
    "condition_occurrence = pd.DataFrame({\n",
    "    'person_id':[1, 1, 1, 1, 2, 2],\n",
    "    'condition_concept_id':[44831230, 2, 3, 4, 44831230, 2],\n",
    "    'condition_start_datetime':[\n",
    "        pd.to_datetime('2017-12-10'),\n",
    "        pd.to_datetime('2017-12-10'),\n",
    "        pd.to_datetime('2017-12-10'),\n",
    "        pd.to_datetime('2017-12-10'),\n",
    "        pd.to_datetime('2017-12-10'),\n",
    "        pd.to_datetime('2017-12-10'),\n",
    "    ]\n",
    "})\n",
    "observation_period = pd.DataFrame({\n",
    "    'person_id':[1, 2],\n",
    "    'observation_period_start_date':[\n",
    "        pd.to_datetime('2015-01-01'),\n",
    "        pd.to_datetime('2017-12-01')\n",
    "    ],\n",
    "    'observation_period_end_date':[\n",
    "        pd.to_datetime('2019-01-01'),\n",
    "        pd.to_datetime('2018-02-01')\n",
    "    ]\n",
    "})\n",
    "drug_exposure = pd.DataFrame({\n",
    "    'person_id':[1, 1, 1, 1, 2, 2],\n",
    "    'drug_concept_id':[10, 20, 30, 40, 10, 20],\n",
    "    'drug_exposure_start_datetime':[\n",
    "        pd.to_datetime('2017-12-10'),\n",
    "        pd.to_datetime('2017-12-10'),\n",
    "        pd.to_datetime('2017-12-10'),\n",
    "        pd.to_datetime('2017-12-10'),\n",
    "        pd.to_datetime('2017-12-10'),\n",
    "        pd.to_datetime('2017-12-10'),\n",
    "    ]\n",
    "})\n",
    "\n",
    "visit_occurrence = pd.DataFrame({\n",
    "    'person_id':[1],\n",
    "    'visit_start_datetime':[\n",
    "        pd.to_datetime('2017-12-10')\n",
    "    ]\n",
    "})\n",
    "visit_occurrence = dd.from_pandas(visit_occurrence, npartitions=1).set_index('person_id')\n",
    "person = dd.from_pandas(person, npartitions=1).set_index('person_id')\n",
    "condition_occurrence = dd.from_pandas(condition_occurrence, npartitions=1).set_index('person_id')\n",
    "observation_period = dd.from_pandas(observation_period, npartitions=1).set_index('person_id')\n",
    "drug_exposure = dd.from_pandas(drug_exposure, npartitions=1).set_index('person_id')\n",
    "measurement = pd.DataFrame()\n",
    "procedure = pd.DataFrame()\n",
    "measurement = dd.from_pandas(measurement, npartitions=1)\n",
    "procedure = dd.from_pandas(procedure, npartitions=1)\n",
    "omop_tables = {\n",
    "    'person':person,\n",
    "    'condition_occurrence':condition_occurrence,\n",
    "    'procedure_occurrence':procedure,\n",
    "    'drug_exposure':drug_exposure,\n",
    "    'visit_occurrence':visit_occurrence,\n",
    "    'observation_period':observation_period,\n",
    "    'measurement':measurement\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def line_generation_preprocess(cohort, ingredient_list, omop_tables):\n",
    "    subjects = cohort.person_id.unique().tolist()\n",
    "    descendants = Descendants()\n",
    "    ingredients = Ingredient()\n",
    "    drug_temp = omop_tables['drug_exposure'].loc[drug_exposure.index.isin(subjects)]\n",
    "    drug_temp = drug_temp[drug_temp['drug_concept_id'].isin(descendants(ingredient_list))]\n",
    "    drug_temp = drug_temp.compute().reset_index()\n",
    "    drug_temp = ingredients(drug_temp)\n",
    "    drug_temp = dd.from_pandas(drug_temp, npartitions=1).set_index('person_id')\n",
    "    last_activity = last_activity_date(cohort, omop_tables)\n",
    "    cohort_enhanced = pd.merge(cohort, last_activity, how='left', on='person_id')\n",
    "    cohort_enhanced = dd.from_pandas(cohort_enhanced, npartitions=1).set_index('person_id')\n",
    "    return drug_temp, cohort_enhanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_temp, cohort_enhanced = line_generation_preprocess(cohort, ingredient_list, omop_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def false_after_lot(x):\n",
    "    x = x.sort_values(by=['time_from_start'])\n",
    "    first_false = x['is_in_line'].idxmin()\n",
    "    if not x['is_in_line'][first_false]:\n",
    "        x.loc[first_false:, 'is_in_line'] = False\n",
    "    return x\n",
    "\n",
    "\n",
    "def is_in_line(drug_code, regimen_codes):\n",
    "    \"\"\"\n",
    "    fluorouracil = 955632\n",
    "    capecitabine = 1337620\n",
    "    leucovorin = 1388796\n",
    "    levoleucovorin = 40168303\n",
    "    \"\"\"\n",
    "    substitutes = {\n",
    "        955632:[955632, 1337620],\n",
    "        1337620:[955632, 1337620],\n",
    "        1388796:[1388796, 40168303],\n",
    "        40168303:[1388796, 40168303]\n",
    "    }\n",
    "    if drug_code not in substitutes:\n",
    "        substitutes[drug_code] = [drug_code]\n",
    "\n",
    "    for substitute in substitutes[drug_code]:\n",
    "        if substitute in regimen_codes:\n",
    "            return True\n",
    "\n",
    "    # Addition of leucovorin or levoleucovorin does not advance the lot\n",
    "    return drug_code in [1388796, 40168303]\n",
    "\n",
    "\n",
    "class LinesOfTherapy():\n",
    "    def __init__(self, drug_temp, cohort, offset=14, nb_of_lines = 3):\n",
    "        self.drug_temp = drug_temp\n",
    "        self.index_date = cohort\n",
    "        self.lines = self.__get_drugs(self.drug_temp, self.index_date, offset)\n",
    "        self.lines = self.lines.persist()\n",
    "        self.lines['line_number'] = 0\n",
    "\n",
    "    def __get_drugs(self, df, index, offset):\n",
    "        df = dd.merge(df, index, how='left', on='person_id')\n",
    "        df = df[(df['drug_exposure_start_datetime'] - df['cohort_start_date']).dt.days >= -offset]\n",
    "        return df[['drug_concept_id', 'drug_exposure_start_datetime']]\n",
    "\n",
    "    def __get_lines(self, df, line_number):\n",
    "        def add_paclitaxel_gemcitabine(x):\n",
    "            \"\"\"\n",
    "            Adding Paclitaxel (1378382) or Gemcitabine (1314924)\n",
    "            does not change the line of therapy\n",
    "            \"\"\"\n",
    "            if (1378382 in x[0]) or (1314924 in x[0]):\n",
    "                g = list(set(x[1]).symmetric_difference(set(x[0])))\n",
    "                if 1378382 in g:\n",
    "                    return np.append(x[0], 1378382)\n",
    "                elif 1314924 in g:\n",
    "                    return np.append(x[0], 1314924)\n",
    "            return x[0]\n",
    "        \n",
    "        df = df.reset_index()\n",
    "        start_line = df.groupby(['person_id'])['drug_exposure_start_datetime'].min().to_frame('start_date')\n",
    "        df = df.merge(start_line, how='left', on='person_id')\n",
    "        df['time_from_start'] = (df['drug_exposure_start_datetime'] - df['start_date']).dt.days\n",
    "        regimen_codes = df[df['time_from_start'] <= 28].groupby('person_id').drug_concept_id.unique().to_frame('regimen_codes_28')\n",
    "        df = df.merge(regimen_codes, how='left', on='person_id')\n",
    "        regimen_codes = df[df['time_from_start'] <= 90].groupby('person_id').drug_concept_id.unique().to_frame('regimen_codes_90')\n",
    "        df = df.merge(regimen_codes, how='left', on='person_id')    \n",
    "        df['regimen_codes'] = df['regimen_codes_28']\n",
    "        df['tmp'] = list(zip(df['regimen_codes_28'], df['regimen_codes_90']))\n",
    "        df['regimen_codes'] = df['tmp'].map(add_paclitaxel_gemcitabine)\n",
    "        df = df.sort_values(by=['person_id', 'time_from_start'])\n",
    "        df['tmp'] = list(zip(df['drug_concept_id'], df['regimen_codes']))\n",
    "        df['is_in_line'] = df['tmp'].map(lambda x: is_in_line(x[0], x[1]))\n",
    "        del df['tmp']\n",
    "        df = df.groupby('person_id').apply(false_after_lot).reset_index(drop=True)\n",
    "        df['line_number'] = line_number\n",
    "        df['line_number'] = df['line_number'].astype(int)\n",
    "        return df\n",
    "    \n",
    "    def __call__(self):\n",
    "        line_number = 1\n",
    "        dfs = []\n",
    "        tmp = []\n",
    "        lines = self.lines\n",
    "        \n",
    "        while line_number != nb_of_lines+1:\n",
    "            df = lines.map_partitions(self.__get_lines, line_number)\n",
    "            temp = df[df['is_in_line'] == True][['person_id', 'start_date', 'regimen_codes', 'line_number']]\n",
    "            lines = df[df['is_in_line'] == False][['person_id', 'drug_concept_id', 'drug_exposure_start_datetime']]\n",
    "            dfs.append(temp)\n",
    "            line_number = line_number + 1\n",
    "\n",
    "        lines_f = dd.concat(dfs)\n",
    "        lines_f['regimen_codes'] = lines_f['regimen_codes'].astype(str)\n",
    "        return lines_f.drop_duplicates()\n",
    "\n",
    "\n",
    "def listToString(s):  \n",
    "    str1 = \", \" \n",
    "    return (str1.join(s))\n",
    "\n",
    "\n",
    "class LineName():\n",
    "    def __init__(self, lot_f, concept_infos):\n",
    "        lot_f[\"regimen_codes\"] = lot_f.regimen_codes.str.replace(\" \", ',')\n",
    "        lot_f[\"regimen_codes\"] = lot_f.regimen_codes.str.replace(\"[\", \"\")\n",
    "        lot_f[\"regimen_codes\"] = lot_f.regimen_codes.str.replace(\"]\", \"\")\n",
    "\n",
    "        for index, row in concept_infos.iterrows():\n",
    "            lot_f[\"regimen_codes\"] = lot_f.regimen_codes.str.replace(str(row['concept_id']), row['concept_name'])\n",
    "        self.lot = lot_f\n",
    "\n",
    "    def __call__(self):\n",
    "        self.lot['regimen_codes_sorted'] = self.lot['regimen_codes'].str.split(',')\n",
    "        self.lot = self.lot.reset_index(drop=True)\n",
    "        for index, row in self.lot.iterrows():\n",
    "            row['regimen_codes_sorted'].sort()\n",
    "            regimen_sorted = listToString(row['regimen_codes_sorted'])\n",
    "            self.lot.loc[index, 'regimen_codes_sorted'] = regimen_sorted\n",
    "\n",
    "        del self.lot['regimen_codes']\n",
    "        self.lot = self.lot.sort_values(by=['person_id', 'line_number'])\n",
    "        self.lot['regimen_codes_sorted'] = self.lot['regimen_codes_sorted'].map(\n",
    "            lambda x:', '.join([l.strip() for l in x.split(',') if l.strip() != '']))\n",
    "\n",
    "        return self.lot.rename(columns={'regimen_codes_sorted':'regimen_name'})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_line_generation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chi-squared test with similar proportions\n",
    "from scipy.stats import chi2_contingency\n",
    "from scipy.stats import chi2\n",
    "\n",
    "xi_results = pd.DataFrame({\n",
    "    'YY': [],\n",
    "    'YN': [],\n",
    "    'NY': [],\n",
    "    'NN': [],\n",
    "    'stat': [],\n",
    "    'p': [],\n",
    "    'dof': [],\n",
    "    'probability': [],\n",
    "    'interpret test-statistic': [],\n",
    "    'interpret p-value': []\n",
    "})\n",
    "table = [[16, 197], [37847, 2286732]]\n",
    "stat, p, dof, expected = chi2_contingency(table)\n",
    "prob = 0.95\n",
    "critical = chi2.ppf(prob, dof)\n",
    "\n",
    "# interpret test-statistic\n",
    "if abs(stat) >= critical:\n",
    "    test_statistic = 'Dependent (reject H0)'\n",
    "else:\n",
    "    test_statistic = 'Independent (fail to reject H0)'\n",
    "\n",
    "# interpret p-value\n",
    "alpha = 1.0 - prob\n",
    "if p <= alpha:\n",
    "    p_value = 'Dependent (reject H0)'\n",
    "else:\n",
    "    p_value = 'Independent (fail to reject H0)'\n",
    "\n",
    "# contingency table\n",
    "xi_add_results = pd.DataFrame({\n",
    "    'YY': [table[0][0]],\n",
    "    'YN': [table[0][1]],\n",
    "    'NY': [table[1][0]],\n",
    "    'NN': [table[1][1]],\n",
    "    'stat': [stat],\n",
    "    'p': [p],\n",
    "    'dof': [dof],\n",
    "    'probability':[prob],\n",
    "    'interpret test-statistic': [test_statistic],\n",
    "    'interpret p-value': [p_value]\n",
    "})\n",
    "\n",
    "xi_results = xi_results.append(xi_add_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xi_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_comorbid_conditions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_concept_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "What is the time for defining a gap?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "era"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "era = t.groupby(['person_id', 'concept_id']).agg({\n",
    "    'start_date':['min', 'max', 'count'],\n",
    "    'gap':['cumsum']\n",
    "})\n",
    "era['era_duration'] = (era['max'] - era['min']).dt.days\n",
    "era = era.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# sudo docker-compose exec --user root  notebook bash\n",
    "# pip install -U scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score, log_loss, classification_report, confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'age_at_index':[18, 28, 8, 48, 58],\n",
    "    'gender = female':[1, 0, 1, 0, 0],\n",
    "    'condition_1':[1, 1, 1, 0, 0],\n",
    "    'condition_2':[1, 1, 1, 0, 0],\n",
    "    'target':[0, 1, 1, 0, 0]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_features_scores(df, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X = df.iloc[:,0:4]\n",
    "scaler = StandardScaler().fit(X)\n",
    "standardized_X = scaler.transform(X)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#X = df.iloc[:,0:4]  #independent columns\n",
    "X = standardized_X\n",
    "y = df.iloc[:,-1]    #target column i.e price range\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "clf = LogisticRegression(random_state=0, solver='lbfgs').fit(X, y)\n",
    "#clf.predict(X)\n",
    "proba = pd.DataFrame(clf.predict_proba(X))[[1]]\n",
    "proba.columns = ['probability']\n",
    "proba['probability'] = proba['probability'].apply(lambda x:round(x, 4))\n",
    "pd.concat([df, proba], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from rwd_analytics.lookups import Descendants, ComorbidConditions\n",
    "\n",
    "def get_features_scores(df, n_features):\n",
    "    X = df.iloc[:,0:n_features]  #independent columns\n",
    "    y = df.iloc[:,-1]    #target column i.e price range\n",
    "    \n",
    "    #apply SelectKBest class to extract top 10 best features\n",
    "    bestfeatures = SelectKBest(score_func=chi2, k=n_features)\n",
    "    fit = bestfeatures.fit(X,y)\n",
    "    dfscores = pd.DataFrame(fit.scores_)\n",
    "    dfcolumns = pd.DataFrame(X.columns)\n",
    "    \n",
    "    #concat two dataframes for better visualization \n",
    "    featureScores = pd.concat([dfcolumns, dfscores], axis=1)\n",
    "\n",
    "    # naming the dataframe columns and rounding results\n",
    "    featureScores.columns = ['Specs', 'Score']\n",
    "    featureScores['Score'] = featureScores['Score'].round(2)\n",
    "    return featureScores.nlargest(n_features, 'Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_features_scores(df, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PredictionModels(df)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionModels():\n",
    "    def __init__(self, df):\n",
    "        a = df[df['target']==1]\n",
    "        b = df[df['target']==0]\n",
    "        print('Subjects in target=1: '+str(len(a)))\n",
    "        print('Subjects in target=0: '+str(len(b)))\n",
    "        \n",
    "        X = df.drop('target', axis=1)\n",
    "        y = df['target']\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, test_size=0.25)\n",
    "        \n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "        \n",
    "        #if (len(a)+len(b))/20 < abs(len(b)-len(a)):\n",
    "        #    print('Imbalanced dataset: resampling train set')\n",
    "        #    print('************************************')\n",
    "        #    c = math.trunc((max(len(a), len(b)) + min(len(a), len(b)))/2)\n",
    "        #    a = self.X_train.sample(n=c, replace=True, random_state=3)\n",
    "        #    b = self.y_train.sample(n=c, replace=True, random_state=3)\n",
    "        #    df = pd.concat([a, b], ignore_index=True)\n",
    "\n",
    "    def __call__(self):\n",
    "        feedback = pd.DataFrame(columns=['Classifier', 'Model Score', 'Accuracy Score'])\n",
    "        classifiers = [\n",
    "            DummyClassifier(strategy='most_frequent', random_state=0),\n",
    "            KNeighborsClassifier(3),\n",
    "            SVC(kernel=\"rbf\", C=0.025, probability=True),\n",
    "            #NuSVC(probability=True),\n",
    "            DecisionTreeClassifier(),\n",
    "            RandomForestClassifier(),\n",
    "            AdaBoostClassifier(),\n",
    "            GradientBoostingClassifier()\n",
    "            ]\n",
    "\n",
    "        for classifier in classifiers:\n",
    "            model = classifier.fit(self.X_train, self.y_train)\n",
    "            y_pred = model.predict(self.X_test)\n",
    "            feedback_temp = pd.DataFrame({\n",
    "                'Classifier':[classifier],\n",
    "                'Model Score':[model.score(self.X_train, self.y_train)],\n",
    "                'Accuracy Score':[accuracy_score(self.y_test, y_pred)]\n",
    "            })\n",
    "            feedback.append(feedback_temp)\n",
    "            print(classifier)\n",
    "            print(\"Training score: %.3f\" % model.score(self.X_train, self.y_train))\n",
    "            print(\"Test score: %.3f\" % accuracy_score(self.y_test, y_pred))\n",
    "            print('*** Confusion matrix ***')\n",
    "            print(confusion_matrix(self.y_test, y_pred))\n",
    "            print('*** Classification report ***')\n",
    "            print(classification_report(self.y_test, y_pred))\n",
    "            print('************************************')\n",
    "        \n",
    "        return feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
